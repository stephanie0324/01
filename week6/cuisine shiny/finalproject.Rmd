---
title: "final project"
author: "stephanie"
date: "2018/8/5"
output: html_document
---
```{r,message=FALSE,warning=FALSE}

library(jsonlite)
library(tidyverse)
library(tidytext)
library(scales)
library(stringr)
library(wordcloud)
library(treemap)
library(text2vec)
library(glmnet)
library(igraph)
library(ggraph)
library(knitr)

rm(list=ls())

fillColor = "#FFA07A"
fillColor2 = "#F1C40F"
fillColorBlue = "#AED6F1"

train <- fromJSON("../data/train.json", flatten = TRUE)
test <- fromJSON("../data/test.json", flatten = TRUE)
train2<-train
```

#Prepare the Ingredients data

```{r,message=FALSE,warning=FALSE}

ingredientscombine <- function(s)
{
  a <- unlist(s)
  return(paste0(a, collapse = '',sep=' '))
}

train$ingredients <- sapply(train$ingredients,ingredientscombine)
train <- train %>%
        rename(text = ingredients)
test$ingredients <- sapply(test$ingredients,ingredientscombine)
test <- test %>%
        rename(text = ingredients)
```
#Italian cuisine Most Common Ingredients

```{r,message=FALSE,warning=FALSE}
most_common_words <- train %>%
    unnest_tokens(word, text) %>%
    filter(!word %in% stop_words$word) %>%
    count(word,sort = TRUE) %>%
    ungroup() %>%
    mutate(word = factor(word, levels = rev(unique(word)))) %>%
    head(20)
most_common_ingredients <- train2 %>% 
  mutate(ingredients = str_split(ingredients, pattern = ",")) %>% 
  unnest(ingredients) %>% 
  mutate(ingredients = gsub(ingredients, pattern = 'c\\(', replacement = "")) %>%
  mutate(ingredients = gsub(ingredients, pattern = '"', replacement = "")) %>%
  mutate(ingredients = trimws(ingredients)) %>%
  group_by(ingredients) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) %>%
  ungroup() %>%
  head(10)

createBarPlotCommonWordsInCuisine = function(train,cuisineName,titleName,fillColorName)
{
  train %>% 
  filter(cuisine == cuisineName) %>%
  mutate(ingredients = str_split(ingredients, pattern = ",")) %>% 
  unnest(ingredients) %>% 
  mutate(ingredients = gsub(ingredients, pattern = 'c\\(', replacement = "")) %>%
  mutate(ingredients = gsub(ingredients, pattern = '"', replacement = "")) %>%
  mutate(ingredients = trimws(ingredients)) %>%
  filter(!ingredients %in% most_common_ingredients$ingredients) %>%
  group_by(ingredients) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) %>%
  ungroup() %>%
  mutate(ingredients = reorder(ingredients,Count)) %>%
  head(10) %>%
  
  
  ggplot(aes(x = ingredients,y = Count)) +
  geom_bar(stat='identity',fill= fillColor2) +
  geom_text(aes(x = ingredients, y = .01, label = paste0("( ",Count," )",sep="")),
            hjust=0, vjust=.5, size = 4, colour = 'black',
            fontface = 'bold') +
  labs(x = 'ingredients', 
       y = 'Count', 
       title = titleName) +
  coord_flip() +
  theme_bw()
  
}

createBarPlotCommonWordsInCuisine(train2,"italian","Most Common Ingredients in Italian Cuisine",fillColor2)

```

##Italian WordCloud

```{r,message=FALSE,warning=FALSE}

createWordCloudWithCuisine = function(train,cuisineName)
{
  train %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!word %in% most_common_words$word) %>%
  filter(cuisine == cuisineName) %>%
  count(word,sort = TRUE) %>%
  ungroup()  %>%
  head(30) %>%
  
  with(wordcloud(word, n, max.words = 30,colors=brewer.pal(8, "Dark2")))
}

createWordCloudWithCuisine(train,"italian")

```

#Mexican cuisine Most Common Ingredients


Most Common **Mexican** ingredients are chilli powder, jalepeno chillies , avocado , ground cumin , sour cream            

```{r,message=FALSE,warning=FALSE}

createBarPlotCommonWordsInCuisine(train2,"mexican","Most Common Ingredients in Mexican Cuisine",fillColor2)

```

##Mexican WordCloud

```{r,message=FALSE,warning=FALSE}

createWordCloudWithCuisine(train,"mexican")

```

#Thai cuisine Most Common Ingredients

Most Common **Thai** ingredients are fish sauce, chilli sauce , vegetable oil , coconut milk , lime             

```{r,message=FALSE,warning=FALSE}

createBarPlotCommonWordsInCuisine(train2,"thai","Most Common Ingredients in Thai Cuisine",fillColor2)

```

##Thai WordCloud

```{r,message=FALSE,warning=FALSE}

createWordCloudWithCuisine(train,"thai")

```


#Moroccan cuisine Most Common Ingredients

Most Common **Moroccan** ingredients are ground cumin,ground cinnamon,ground ginger,paprika , carrots            

```{r,message=FALSE,warning=FALSE}

createBarPlotCommonWordsInCuisine(train2,"moroccan","Most Common Ingredients in Moroccan Cuisine",fillColor2)

```

##Moroccan WordCloud

```{r,message=FALSE,warning=FALSE}

createWordCloudWithCuisine(train,"moroccan")

```


#TF-IDF

We wish to find out the important words which are in the **Cuisines**. Example for your young child , the most important word is **mom**. Example for a bar tender , important words would be related to **drinks**.

We would explore this using a fascinating concept known as **Term Frequency - Inverse Document Frequency**. Quite a mouthful, but we will unpack it and clarify each and every term. 


A **document** in this case is the set of lines associated with a **Cuisine**.Therefore we have different **documents** for each **cuisine**.

From the book [5 Algorithms Every Web Developer Can Use and Understand](https://lizrush.gitbooks.io/algorithms-for-webdevs-ebook/content/chapters/tf-idf.html)       


>    TF-IDF computes a weight which represents the importance of a term inside a document. 

>    It does this by comparing the frequency of usage inside an individual document as opposed to the entire data set (a collection of documents).
The importance increases proportionally to the number of times a word appears in the individual document itself--this is called Term Frequency. However, if multiple documents contain the same word many times then you run into a problem. That's why TF-IDF also offsets this value by the frequency of the term in the entire document set, a value called Inverse Document Frequency.


## The Math
>  TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)         
IDF(t) = log_e(Total number of documents / Number of documents with term t in it).         
Value = TF * IDF


##Twenty Most Important Words

```{r,message=FALSE,warning=FALSE}

plotMostImportantWords <- function(train) {
  trainWords <- train %>%
    unnest_tokens(word, text) %>%
    count(cuisine, word, sort = TRUE) %>%
    ungroup()
  
  total_words <- trainWords %>% 
    group_by(cuisine) %>% 
    summarize(total = sum(n))
  
  trainWords <- left_join(trainWords, total_words)
  
  #Now we are ready to use the bind_tf_idf which computes the tf-idf for each term. 
  trainWords <- trainWords %>%
    filter(!is.na(cuisine)) %>%
    bind_tf_idf(word, cuisine, n)
  
     
  plot_trainWords <- trainWords %>%
    arrange(desc(tf_idf)) %>%
    mutate(word = factor(word, levels = rev(unique(word))))
  
  return(plot_trainWords)
}

plot_trainWords <- plotMostImportantWords(train)

 plot_trainWords %>% 
    top_n(20) %>%
    ggplot(aes(word, tf_idf)) +
    geom_col(fill = fillColor2) +
    labs(x = NULL, y = "tf-idf") +
    coord_flip() +
    theme_bw()

```

##Most Important Ingredients in **Italian Cuisine**

```{r,message=FALSE,warning=FALSE}

 plotMostImportantIngredientsInCuisine <- function(plot_trainWords, cuisineName,fillColorName = fillColor) {
   plot_trainWords %>% 
      filter(cuisine == cuisineName) %>%
      top_n(10) %>%
      ggplot(aes(word, tf_idf)) +
      geom_col(fill = fillColorName) +
      labs(x = NULL, y = "tf-idf") +
      coord_flip() +
      theme_bw()
 }

plotMostImportantIngredientsInCuisine(plot_trainWords,"italian",fillColor)

```

##Most Important Ingredients in **Mexican Cuisine**

```{r,message=FALSE,warning=FALSE}

 plotMostImportantIngredientsInCuisine(plot_trainWords,"mexican",fillColor2)

```

##Most Important Ingredients in **Thai Cuisine**

```{r,message=FALSE,warning=FALSE}

 plotMostImportantIngredientsInCuisine(plot_trainWords,"thai",fillColor)

```

##Most Important Ingredients in **Moroccan Cuisine**

```{r,message=FALSE,warning=FALSE}

 plotMostImportantIngredientsInCuisine(plot_trainWords,"moroccan",fillColor2)

```

#Most Common Bigrams

A **Bigram** is a collection of Two words. We examine the most common Bigrams and plot them in a bar plot.

The most common ingredients with Two words are olive oil, black pepper ,garlic cloves , ground black  and soy sauce            

```{r,message=FALSE,warning=FALSE}

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}


visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
  
}

visualize_bigrams_individual <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a,end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

train %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  unite(bigramWord, word1, word2, sep = " ") %>%
  group_by(bigramWord) %>%
  tally() %>%
  ungroup() %>%
  arrange(desc(n)) %>%
  mutate(bigramWord = reorder(bigramWord,n)) %>%
  head(10) %>%
  
  ggplot(aes(x = bigramWord,y = n)) +
  geom_bar(stat='identity',colour="white", fill = fillColor2) +
  geom_text(aes(x = bigramWord, y = 1, label = paste0("(",n,")",sep="")),
            hjust=0, vjust=.5, size = 4, colour = 'black',
            fontface = 'bold') +
  labs(x = 'Bigram', 
       y = 'Count', 
       title = 'Bigram and Count') +
  coord_flip() + 
  theme_bw()

```

#Most Common Trigrams

A **Trigram** is a collection of Three words. We examine the most common Trigrams and plot them in a bar plot.The most common ingredients with Three words are ground black pepper, virgin olive oil ,extra virgin olive oil , red bell pepper , grated parmesan cheese


```{r,message=FALSE,warning=FALSE}

train %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2","word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  unite(trigramWord, word1, word2, word3,sep = " ") %>%
  group_by(trigramWord) %>%
  tally() %>%
  ungroup() %>%
  arrange(desc(n)) %>%
  mutate(trigramWord = reorder(trigramWord,n)) %>%
  head(10) %>%
  
  ggplot(aes(x = trigramWord,y = n)) +
  geom_bar(stat='identity',colour="white", fill = fillColor2) +
  geom_text(aes(x = trigramWord, y = 1, label = paste0("(",n,")",sep="")),
            hjust=0, vjust=.5, size = 4, colour = 'black',
            fontface = 'bold') +
  labs(x = 'Trigram', 
       y = 'Count', 
       title = 'Trigram and Count') +
  coord_flip() + 
  theme_bw()

```

#Relationship among Italian ingredients

We will explore the relationship between Italian ingredients               

```{r,message=FALSE,warning=FALSE}

trainWords <- train %>% 
  filter(cuisine == "italian") %>%
  count_bigrams()

head(trainWords)

trainWords %>%
  filter(n > 200) %>%
  visualize_bigrams()

```


##Relationship among Italian ingredient - Cheese

**Cheese** is an important ingredient of italian cuisine. The variety of colors of cheese such as **red, green, blue, yellow** are shown in the network diagram.  The differentiating ingredients of Italian cuisine such as **ricotta** is also associated with cheese.           

```{r,message=FALSE,warning=FALSE}

trainWords %>%
  filter( word1 == "cheese" | word2 == "cheese") %>%
  filter( n >= 20) %>%
  visualize_bigrams()

```


#Relationship among Mexican ingredients

We will explore the relationship between Mexican ingredients               

```{r,message=FALSE,warning=FALSE}

trainWords <- train %>% 
  filter(cuisine == "mexican") %>%
  count_bigrams()

trainWords %>%
  filter(n > 200) %>%
  visualize_bigrams()

```





#Modelling using the text2vec package

We create a vocabulary-based DTM. Here we collect unique terms from all documents and mark each of them with a unique ID using the create_vocabulary() function. We use an iterator to create the vocabulary. We also prune the vocabulary to reduce the terms in the matrix.                       


```{r,message=FALSE,warning=FALSE}

rm(trainWords)
gc()


prep_fun  = function(x) {
  stringr::str_replace_all(tolower(x), "[^[:alpha:]]", " ")
}

tok_fun = word_tokenizer

it_train = itoken(train$text, 
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun, 
                  ids = train$id, 
                  progressbar = FALSE)



it_test = test$text %>% 
  prep_fun %>% 
  tok_fun %>% 
  itoken(ids = test$id,  progressbar = FALSE)


NFOLDS = 4
vocab = create_vocabulary(it_train, ngram = c(1L, 3L))
vocab = vocab %>% prune_vocabulary(term_count_min = 10, 
                                   doc_proportion_max = 0.5,
                                   doc_proportion_min = 0.01,vocab_term_max = 5000)

trigram_vectorizer = vocab_vectorizer(vocab)

dtm_train = create_dtm(it_train, trigram_vectorizer)

```


##Inspect the vocabulary

```{r,message=FALSE,warning=FALSE}

vocab

```

##Inspect the Document Term Matrix

```{r,message=FALSE,warning=FALSE}

dim(dtm_train)

```

##TF-IDF

```{r,message=FALSE,warning=FALSE}

# define tfidf model
tfidf = TfIdf$new(norm = "l2", sublinear_tf = T)

# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train, tfidf)

# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf = create_dtm(it_test, trigram_vectorizer)

dtm_test_tfidf = transform(dtm_test_tfidf, tfidf)

rm(dtm_train)
gc()


```



##Build the Multinomial Logistic Regression Model

```{r,message=FALSE,warning=FALSE}


glmnet_classifier = cv.glmnet(x = dtm_train_tfidf, y = train[['cuisine']], 
                              family = 'multinomial', 
                              alpha = 1,
                              type.measure = "class",
                              nfolds = NFOLDS,
                              thresh = 1e-3,
                              maxit = 1e3)


```

##Predict using the Multinomial Logistic Regression Model

```{r,message=FALSE,warning=FALSE}

predictions = data.frame(id=test$id,cuisine = predict(glmnet_classifier, dtm_test_tfidf,type="class"))

options(scipen = 999)

predictions <- predictions %>%
  rename(cuisine = X1)

predictions$id = as.numeric(predictions$id)

write.csv(predictions, 'glmnet.csv', row.names = F)

```

#Word Embeddings

**Acknowledgement** -  `Matt Motoki's` kernel

**Global Vectors for Word Representation**

The literature from the Glove Page 

> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.



##Create Word Vectors thru Glove Embeddings


```{r,message=FALSE,warning=FALSE}

vocab2 <- create_vocabulary(it_train)

vectorizer <- vocab_vectorizer(vocab2)

# use window of 4 for context words
tcm <- create_tcm(it_train, vectorizer, skip_grams_window=4L)

# create glove vectors
glove <- GlobalVectors$new(word_vectors_size=100, vocabulary=vocab2, x_max=100)
invisible(capture.output(wv_matrix <- glove$fit_transform(tcm, n_iter=500)))

```

##Creating the Cuisine matrix

The Cuisine Matrix is being created using `Glove`          

```{r,message=FALSE,warning=FALSE}

# lookup cuisines
cuisine_list <- unique(train$cuisine)

cuisine_matrix <- wv_matrix[row.names(wv_matrix) %in% cuisine_list, ]

# normalize vectors
normalize_l2 <- function(x) {x / sqrt(sum(x^2))}
cuisine_matrix <- t(apply(cuisine_matrix, 1, normalize_l2))

    cuisine_matrix %>%
    as.data.frame() %>%
    head() %>%
    kable()

```


##Similiarities between cuisines

The similiarities between the cuisine and their next best matches are shown in order in the following sections.

###Italian 

Italian cuisine is similiar to french, greek ,spanish , chinese and thai.       

```{r,message=FALSE,warning=FALSE}

getSimiliarCuisines <- function(cuisineName) {
  sim <- cuisine_matrix[cuisineName,] %*% t(cuisine_matrix)  
  
  sort(sim[1,],decreasing= T) %>%
    as.data.frame() %>%
    head() %>%
    kable()
}

getSimiliarCuisines("italian")

```

###Mexican 

```{r,message=FALSE,warning=FALSE}

getSimiliarCuisines("mexican")

```

###Thai 

```{r,message=FALSE,warning=FALSE}

getSimiliarCuisines("thai")

```

###Moroccan 

```{r,message=FALSE,warning=FALSE}

getSimiliarCuisines("moroccan")

```

###Indian 

```{r,message=FALSE,warning=FALSE}

getSimiliarCuisines("indian")

```

###Greek 

```{r,message=FALSE,warning=FALSE}

getSimiliarCuisines("greek")

```